{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"NguyenVanHieu1605/NER-VN\"\n",
    "labels_name = {'O':0, 'B-PER':1, 'I-PER':2, 'B-ORG':3, 'I-ORG':4, 'B-LOC':5, 'I-LOC':6, 'B-MISC':7, 'I-MISC':8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_in_directory(directory_path):\n",
    "    tokens_batches, NE_labels_batches, nested_NE_labels_batches = [], [], []\n",
    "    tokens, NE_labels, nested_NE_labels = [], [], []\n",
    "    full_path = os.path.join(os.getcwd(), directory_path)\n",
    "    txt_files = glob.glob(os.path.join(full_path, '*.txt'))\n",
    "    for file_path in txt_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines[3:]:\n",
    "                line = line.strip()\n",
    "                if not line or line == '<s>':\n",
    "                    continue\n",
    "                if line == '</s>':\n",
    "                    tokens_batches.append(tokens)\n",
    "                    NE_labels_batches.append(NE_labels)\n",
    "                    nested_NE_labels_batches.append(nested_NE_labels) \n",
    "                    tokens, NE_labels, nested_NE_labels = [], [], []\n",
    "                else:\n",
    "                    col = line.split()\n",
    "                    tokens.append(col[0].replace('_', ' '))\n",
    "                    NE_labels.append(labels_name.get(col[3]))\n",
    "                    nested_NE_labels.append(labels_name.get(col[4]))\n",
    "\n",
    "    return {'tokens': tokens_batches,'NE_labels': NE_labels_batches,'nested_NE_labels': nested_NE_labels_batches,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = Dataset.from_dict(read_files_in_directory('VLSP2016/test'))\n",
    "train_dict = Dataset.from_dict(read_files_in_directory('VLSP2016/train'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict, validation_dict = train_test_split(train_dict, test_size=0.2, random_state=42)\n",
    "train_dataset = Dataset.from_dict(train_dict)\n",
    "validation_dataset = Dataset.from_dict(validation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0eaee7064484398a74507d86a873809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a808bb4208c472684a38a8322dded08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6b806206ec4842b19d878f3cd0447f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32cad20634a54c64a17bcd856454b54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72f271b62b34801a1cf9d9d13525635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ed83552ab24010bcf24511693c9855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c20599e3f7b4a64a81fc7bb8ca1c570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating downloaded metadata with the new split.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4770021107489d82db65da07dfd602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b62c3339f0140b5b355ab1876fba593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12eb2a6be5945fb9b525cb7bea33d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e56128d09040c4b9a2564dd8c77b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/503 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating downloaded metadata with the new split.\n"
     ]
    }
   ],
   "source": [
    "train_dataset.push_to_hub(dataset_name, split=str(\"train\"))\n",
    "validation_dataset.push_to_hub(dataset_name, split=str(\"valid\"))\n",
    "test_dataset.push_to_hub(dataset_name, split=str(\"test\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
